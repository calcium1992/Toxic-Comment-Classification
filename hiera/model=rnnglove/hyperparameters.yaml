preprocessing:
  input_convertor: 'nn_vectorization'
  pretrained_embedding_file: '/Users/songyihe/Documents/Study/AI Projects/large-datasets/toxic-comments-classification/glove/glove.twitter.27B.200d.txt'
  embedding_dim: 200
  maxlen: 128
training:
  model_name: 'rnnglove'
  learning_rate: 0.8
  embedding_dim: 200
  maxlen: 128
