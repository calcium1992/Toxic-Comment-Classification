preprocessing:
  input_convertor: 'nn_vectorization'
  pretrained_embedding_file: '/Users/songyihe/Documents/Study/AI Projects/large-datasets/toxic-comments/glove/glove.twitter.27B.200d.txt'
  embedding_dim: 200
  maxlen: 128
training:
  model_name: 'transformer'
  learning_rate: 0.8
  embedding_dim: 200
  num_heads: 4
  feed_forward_dim: 128
  maxlen: 128
  dropout_rate: 0.1
  epochs: 10