preprocessing:
  input_convertor: 'nn_vectorization'
  pretrained_embedding: '/Users/songyihe/Documents/Study/AI Projects/large-datasets/toxic-comments/glove.twitter.27B.200d.txt'
  maxlen: 128
training:
  model_name: 'cnn'
  learning_rate: 0.8
  embedding_dim: 200
  maxlen: 128
